W0609 09:33:45.225000 192212 torch/distributed/run.py:792] 
W0609 09:33:45.225000 192212 torch/distributed/run.py:792] *****************************************
W0609 09:33:45.225000 192212 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0609 09:33:45.225000 192212 torch/distributed/run.py:792] *****************************************
2025-06-09 09:34:12,378 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:34:12,378 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:34:12,385 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:34:12,386 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:34:12,388 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:34:12,388 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:34:12,390 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:34:12,392 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
[rank0]:[W609 09:34:13.726922221 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W609 09:34:13.727247966 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W609 09:34:13.727762819 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W609 09:34:13.740109102 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W609 09:34:13.740160061 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W609 09:34:13.740186870 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W609 09:34:13.740451487 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W609 09:34:13.740765872 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-06-09 09:34:30,344 - INFO - Initializing new model
2025-06-09 09:34:30,344 - INFO - Initializing new model
2025-06-09 09:34:30,344 - INFO - Initializing new model
2025-06-09 09:34:30,344 - INFO - Initializing new model
2025-06-09 09:34:30,345 - INFO - Initializing new model
2025-06-09 09:34:30,345 - INFO - Initializing new model
2025-06-09 09:34:30,345 - INFO - Initializing new model
2025-06-09 09:34:30,345 - INFO - Initializing new model
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 128.54it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 144.89it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 127.59it/s]


Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 141.45it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 145.09it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 141.20it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 141.53it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 139.10it/s]
2025-06-09 09:34:32,775 - INFO - Wrapping the model with parallelism
2025-06-09 09:34:32,775 - INFO - Wrapping the model with parallelism
2025-06-09 09:34:32,775 - INFO - Wrapping the model with parallelism
2025-06-09 09:34:32,775 - INFO - Wrapping the model with parallelism
2025-06-09 09:34:32,775 - INFO - Wrapping the model with parallelism
2025-06-09 09:34:32,775 - INFO - Wrapping the model with parallelism
2025-06-09 09:34:32,775 - INFO - Wrapping the model with parallelism
2025-06-09 09:34:32,775 - INFO - Wrapping the model with parallelism
2025-06-09 09:34:36,539 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:34:36,539 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:34:36,539 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:34:36,539 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:34:36,539 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:34:36,539 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:34:36,539 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:34:36,539 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:34:39,017 - INFO - Loaded 35210 examples
2025-06-09 09:34:39,017 - INFO - Loaded 35210 examples
2025-06-09 09:34:39,017 - INFO - Training ...
2025-06-09 09:34:39,017 - INFO - Training ...
2025-06-09 09:34:39,044 - INFO - Loaded 35210 examples
2025-06-09 09:34:39,044 - INFO - Training ...
2025-06-09 09:34:39,053 - INFO - Loaded 35210 examples
2025-06-09 09:34:39,054 - INFO - Training ...
2025-06-09 09:34:39,054 - INFO - Loaded 35210 examples
2025-06-09 09:34:39,055 - INFO - Training ...
2025-06-09 09:34:39,120 - INFO - Loaded 35210 examples
2025-06-09 09:34:39,120 - INFO - Training ...
2025-06-09 09:34:39,203 - INFO - Loaded 35210 examples
2025-06-09 09:34:39,203 - INFO - Training ...
2025-06-09 09:34:39,246 - INFO - Loaded 35210 examples
2025-06-09 09:34:39,246 - INFO - Training ...
  0%|          | 0/64 [00:00<?, ?it/s]  2%|▏         | 1/64 [00:06<06:22,  6.08s/it]  3%|▎         | 2/64 [00:11<05:47,  5.60s/it]  5%|▍         | 3/64 [00:16<05:33,  5.46s/it]  6%|▋         | 4/64 [00:21<05:24,  5.40s/it]  8%|▊         | 5/64 [00:27<05:17,  5.38s/it]  9%|▉         | 6/64 [00:32<05:11,  5.37s/it] 11%|█         | 7/64 [00:37<05:05,  5.36s/it] 12%|█▎        | 8/64 [00:43<05:00,  5.36s/it] 14%|█▍        | 9/64 [00:48<04:54,  5.35s/it] 16%|█▌        | 10/64 [00:54<04:48,  5.35s/it] 17%|█▋        | 11/64 [00:59<04:43,  5.35s/it] 19%|█▉        | 12/64 [01:04<04:37,  5.34s/it] 20%|██        | 13/64 [01:10<04:32,  5.34s/it] 22%|██▏       | 14/64 [01:15<04:27,  5.34s/it] 23%|██▎       | 15/64 [01:20<04:22,  5.35s/it] 25%|██▌       | 16/64 [01:26<04:16,  5.35s/it] 27%|██▋       | 17/64 [01:31<04:11,  5.35s/it] 28%|██▊       | 18/64 [01:36<04:06,  5.36s/it] 30%|██▉       | 19/64 [01:42<04:01,  5.37s/it] 31%|███▏      | 20/64 [01:47<03:56,  5.37s/it] 33%|███▎      | 21/64 [01:52<03:50,  5.37s/it] 34%|███▍      | 22/64 [01:58<03:45,  5.37s/it] 36%|███▌      | 23/64 [02:03<03:39,  5.36s/it] 38%|███▊      | 24/64 [02:08<03:34,  5.35s/it] 39%|███▉      | 25/64 [02:14<03:28,  5.35s/it] 41%|████      | 26/64 [02:19<03:23,  5.36s/it] 42%|████▏     | 27/64 [02:25<03:18,  5.36s/it] 44%|████▍     | 28/64 [02:30<03:13,  5.36s/it] 45%|████▌     | 29/64 [02:35<03:07,  5.36s/it] 47%|████▋     | 30/64 [02:41<03:02,  5.37s/it] 48%|████▊     | 31/64 [02:46<02:57,  5.37s/it] 50%|█████     | 32/64 [02:51<02:51,  5.37s/it] 52%|█████▏    | 33/64 [02:57<02:46,  5.37s/it] 53%|█████▎    | 34/64 [03:02<02:40,  5.37s/it] 55%|█████▍    | 35/64 [03:08<02:35,  5.36s/it] 56%|█████▋    | 36/64 [03:13<02:30,  5.36s/it] 58%|█████▊    | 37/64 [03:18<02:24,  5.36s/it] 59%|█████▉    | 38/64 [03:24<02:19,  5.36s/it] 61%|██████    | 39/64 [03:29<02:14,  5.37s/it] 62%|██████▎   | 40/64 [03:34<02:08,  5.37s/it] 64%|██████▍   | 41/64 [03:40<02:03,  5.37s/it] 66%|██████▌   | 42/64 [03:45<01:58,  5.37s/it] 67%|██████▋   | 43/64 [03:50<01:52,  5.37s/it] 69%|██████▉   | 44/64 [03:56<01:47,  5.37s/it] 70%|███████   | 45/64 [04:01<01:42,  5.37s/it] 72%|███████▏  | 46/64 [04:07<01:36,  5.37s/it]                                               {'loss': 1.9519, 'grad_norm': 0.9960032105445862, 'learning_rate': 5.9375e-05, 'epoch': 1.0}
 72%|███████▏  | 46/64 [04:07<01:36,  5.37s/it]/home/tonoprishvili/.local/lib/python3.12/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: d061df27-b178-4793-bf94-81ea09af863d)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B.
  warnings.warn(
/home/tonoprishvili/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B - will assume that the vocabulary was not modified.
  warnings.warn(
 73%|███████▎  | 47/64 [04:23<02:26,  8.64s/it] 75%|███████▌  | 48/64 [04:29<02:05,  7.86s/it] 77%|███████▋  | 49/64 [04:35<01:49,  7.30s/it] 78%|███████▊  | 50/64 [04:41<01:36,  6.92s/it] 80%|███████▉  | 51/64 [04:47<01:26,  6.65s/it] 81%|████████▏ | 52/64 [04:53<01:17,  6.46s/it] 83%|████████▎ | 53/64 [04:59<01:09,  6.33s/it] 84%|████████▍ | 54/64 [05:05<01:02,  6.23s/it] 86%|████████▌ | 55/64 [05:11<00:55,  6.16s/it] 88%|████████▊ | 56/64 [05:17<00:49,  6.13s/it] 89%|████████▉ | 57/64 [05:23<00:42,  6.10s/it] 91%|█████████ | 58/64 [05:29<00:36,  6.08s/it] 92%|█████████▏| 59/64 [05:35<00:30,  6.06s/it] 94%|█████████▍| 60/64 [05:41<00:24,  6.05s/it] 95%|█████████▌| 61/64 [05:47<00:18,  6.04s/it] 97%|█████████▋| 62/64 [05:53<00:12,  6.03s/it] 98%|█████████▊| 63/64 [05:59<00:06,  6.03s/it]100%|██████████| 64/64 [06:05<00:00,  6.02s/it]                                               {'loss': 0.3243, 'grad_norm': 0.9609066247940063, 'learning_rate': 3.125e-06, 'epoch': 1.39}
100%|██████████| 64/64 [06:05<00:00,  6.02s/it]                                               {'train_runtime': 367.137, 'train_samples_per_second': 133.879, 'train_steps_per_second': 0.174, 'train_loss': 1.4941667765378952, 'epoch': 1.39}
100%|██████████| 64/64 [06:06<00:00,  6.02s/it]100%|██████████| 64/64 [06:06<00:00,  5.72s/it]
2025-06-09 09:40:50,232 - INFO - Total training time: 367.56 seconds
2025-06-09 09:40:50,473 - INFO - Checkpointing from local_rank = 0 ...
2025-06-09 09:40:50,596 - INFO - DONE: local_rank = 5
2025-06-09 09:40:50,608 - INFO - DONE: local_rank = 6
2025-06-09 09:40:50,610 - INFO - DONE: local_rank = 2
2025-06-09 09:40:50,661 - INFO - DONE: local_rank = 3
2025-06-09 09:40:50,663 - INFO - DONE: local_rank = 1
2025-06-09 09:40:50,731 - INFO - DONE: local_rank = 4
2025-06-09 09:40:50,745 - INFO - DONE: local_rank = 7
2025-06-09 09:40:51,026 - INFO - DONE: local_rank = 0
