W0609 09:06:48.156000 68168 torch/distributed/run.py:792] 
W0609 09:06:48.156000 68168 torch/distributed/run.py:792] *****************************************
W0609 09:06:48.156000 68168 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0609 09:06:48.156000 68168 torch/distributed/run.py:792] *****************************************
2025-06-09 09:07:13,063 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:07:13,085 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:07:13,114 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:07:13,114 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:07:13,116 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:07:13,124 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:07:13,125 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
2025-06-09 09:07:13,125 - INFO - Cache directories configured at: /home/tonoprishvili/hf_cache
[rank4]:[W609 09:07:14.284502563 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W609 09:07:14.394177349 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W609 09:07:14.436220993 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W609 09:07:14.484108549 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W609 09:07:14.484468814 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W609 09:07:14.522398778 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W609 09:07:14.546738386 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W609 09:07:15.093773254 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-06-09 09:07:31,155 - INFO - Initializing new model
2025-06-09 09:07:31,155 - INFO - Initializing new model
2025-06-09 09:07:31,155 - INFO - Initializing new model
2025-06-09 09:07:31,155 - INFO - Initializing new model
2025-06-09 09:07:31,155 - INFO - Initializing new model
2025-06-09 09:07:31,155 - INFO - Initializing new model
2025-06-09 09:07:31,155 - INFO - Initializing new model
2025-06-09 09:07:31,155 - INFO - Initializing new model
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 139.83it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 139.79it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 140.38it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 142.73it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 141.16it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 144.27it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 147.77it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 138.00it/s]
2025-06-09 09:07:32,872 - INFO - Wrapping the model with parallelism
2025-06-09 09:07:32,872 - INFO - Wrapping the model with parallelism
2025-06-09 09:07:32,872 - INFO - Wrapping the model with parallelism
2025-06-09 09:07:32,872 - INFO - Wrapping the model with parallelism
2025-06-09 09:07:32,872 - INFO - Wrapping the model with parallelism
2025-06-09 09:07:32,872 - INFO - Wrapping the model with parallelism
2025-06-09 09:07:32,872 - INFO - Wrapping the model with parallelism
2025-06-09 09:07:32,872 - INFO - Wrapping the model with parallelism
2025-06-09 09:07:36,577 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:07:36,577 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:07:36,577 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:07:36,577 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:07:36,577 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:07:36,577 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:07:36,577 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:07:36,578 - INFO - Loading dataset TornikeO/cosmos_qa (all split)
2025-06-09 09:07:39,329 - INFO - Loaded 35210 examples
2025-06-09 09:07:39,330 - INFO - Training ...
2025-06-09 09:07:39,457 - INFO - Loaded 35210 examples
2025-06-09 09:07:39,458 - INFO - Training ...
2025-06-09 09:07:39,472 - INFO - Loaded 35210 examples
2025-06-09 09:07:39,472 - INFO - Training ...
2025-06-09 09:07:39,552 - INFO - Loaded 35210 examples
2025-06-09 09:07:39,553 - INFO - Training ...
2025-06-09 09:07:39,792 - INFO - Loaded 35210 examples
2025-06-09 09:07:39,792 - INFO - Training ...
2025-06-09 09:07:40,031 - INFO - Loaded 35210 examples
2025-06-09 09:07:40,031 - INFO - Training ...
2025-06-09 09:07:41,986 - INFO - Loaded 35210 examples
2025-06-09 09:07:41,986 - INFO - Training ...
2025-06-09 09:07:42,019 - INFO - Loaded 35210 examples
2025-06-09 09:07:42,019 - INFO - Training ...
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:04<02:27,  4.77s/it]  6%|▋         | 2/32 [00:09<02:17,  4.58s/it]  9%|▉         | 3/32 [00:13<02:10,  4.51s/it] 12%|█▎        | 4/32 [00:18<02:05,  4.49s/it] 16%|█▌        | 5/32 [00:22<02:00,  4.48s/it] 19%|█▉        | 6/32 [00:27<01:56,  4.47s/it] 22%|██▏       | 7/32 [00:31<01:51,  4.47s/it] 25%|██▌       | 8/32 [00:35<01:47,  4.47s/it] 28%|██▊       | 9/32 [00:40<01:42,  4.47s/it] 31%|███▏      | 10/32 [00:44<01:38,  4.47s/it] 34%|███▍      | 11/32 [00:49<01:34,  4.48s/it] 38%|███▊      | 12/32 [00:53<01:29,  4.48s/it] 41%|████      | 13/32 [00:58<01:25,  4.48s/it] 44%|████▍     | 14/32 [01:02<01:20,  4.48s/it] 47%|████▋     | 15/32 [01:07<01:16,  4.49s/it] 50%|█████     | 16/32 [01:11<01:11,  4.49s/it] 53%|█████▎    | 17/32 [01:16<01:07,  4.49s/it] 56%|█████▋    | 18/32 [01:20<01:02,  4.49s/it] 59%|█████▉    | 19/32 [01:25<00:58,  4.49s/it] 62%|██████▎   | 20/32 [01:29<00:53,  4.50s/it] 66%|██████▌   | 21/32 [01:34<00:49,  4.50s/it] 69%|██████▉   | 22/32 [01:38<00:44,  4.50s/it] 72%|███████▏  | 23/32 [01:43<00:40,  4.50s/it] 75%|███████▌  | 24/32 [01:47<00:35,  4.50s/it] 78%|███████▊  | 25/32 [01:52<00:31,  4.50s/it] 81%|████████▏ | 26/32 [01:56<00:27,  4.50s/it] 84%|████████▍ | 27/32 [02:01<00:22,  4.50s/it] 88%|████████▊ | 28/32 [02:05<00:18,  4.50s/it] 91%|█████████ | 29/32 [02:10<00:13,  4.50s/it] 94%|█████████▍| 30/32 [02:14<00:09,  4.51s/it] 97%|█████████▋| 31/32 [02:19<00:04,  4.52s/it]100%|██████████| 32/32 [02:23<00:00,  4.52s/it]                                               {'loss': 2.7104, 'grad_norm': 0.7106104493141174, 'learning_rate': 6.25e-06, 'epoch': 0.7}
100%|██████████| 32/32 [02:23<00:00,  4.52s/it]                                               {'train_runtime': 145.4189, 'train_samples_per_second': 169.001, 'train_steps_per_second': 0.22, 'train_loss': 2.7103817462921143, 'epoch': 0.7}
100%|██████████| 32/32 [02:24<00:00,  4.52s/it]100%|██████████| 32/32 [02:24<00:00,  4.51s/it]
2025-06-09 09:10:08,754 - INFO - Total training time: 145.74 seconds
2025-06-09 09:10:08,770 - INFO - DONE: local_rank = 6
2025-06-09 09:10:08,811 - INFO - DONE: local_rank = 4
2025-06-09 09:10:08,812 - INFO - DONE: local_rank = 7
2025-06-09 09:10:09,007 - INFO - Checkpointing from local_rank = 0 ...
2025-06-09 09:10:09,215 - INFO - DONE: local_rank = 5
2025-06-09 09:10:09,233 - INFO - DONE: local_rank = 2
2025-06-09 09:10:09,239 - INFO - DONE: local_rank = 3
2025-06-09 09:10:09,251 - INFO - DONE: local_rank = 1
2025-06-09 09:10:09,712 - INFO - DONE: local_rank = 0
