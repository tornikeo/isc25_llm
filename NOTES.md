# Things to try

Teams are encouraged to explore various optimization techniques, including but not limited to:

- Model optimization (parallelism, quantization, etc).
- LoRA configurations (parameter tuning or any type of LoRA (qLoRA, DoRA, etc))
- Training strategies (learning rates, batch sizes)
- System optimizations (memory management, I/O optimization)
- Hardware-specific optimizations
- Using TransformerEngine.

Other PyTorch-based extensions.

# Things to keep fixed

The only constraints are:
- Must use PyTorch as the base framework
- Must maintain the provided code structure (what does this mean, exactly?)
- Must be reproducible on competition hardware
- Must maintain the same loggings and checkpointing